{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class EaterEnvironment:\n",
        "    def __init__(self, mover_policy, grid_size=7, g1_state=37, g2_state=49, initial_bananas_g1=100, initial_bananas_g2=100):\n",
        "        self.mover_policy = mover_policy\n",
        "        self.grid_size = grid_size\n",
        "        self.g1 = self.state_to_position(g1_state)\n",
        "        self.g2 = self.state_to_position(g2_state)\n",
        "        self.bananas_g1 = initial_bananas_g1\n",
        "        self.bananas_g2 = initial_bananas_g2\n",
        "        self.actions = [\n",
        "            (1, 0), (0, 1), (0.5, 0.5), (0.2, 0.8), (0.6, 0.4),\n",
        "            (0.8, 0.2), (0.4, 0.6), (0.7, 0.3), (0.3, 0.7)\n",
        "        ]\n",
        "        self.initial_mover_state = 1\n",
        "        self.current_mover_state = self.initial_mover_state\n",
        "        self.current_mover_action = 0\n",
        "        self.done = False\n",
        "        self.action_history = []\n",
        "        self.min_distance_to_g1 = float('inf')\n",
        "        self.min_distance_to_g2 = float('inf')\n",
        "        self.deception_detected = False\n",
        "        self.assumed_true_goal = None  # Assumed true goal after deception is detected\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_mover_state = 1\n",
        "        self.current_mover_action = random.choice(range(len(self.actions)))\n",
        "        self.done = False\n",
        "        self.action_history.clear()\n",
        "        self.min_distance_to_g1 = float('inf')\n",
        "        self.min_distance_to_g2 = float('inf')\n",
        "        self.deception_detected = False\n",
        "        self.assumed_true_goal = None\n",
        "        return (self.current_mover_state, self.current_mover_action)\n",
        "\n",
        "    # Assume other methods like state_to_position, calculate_distance, etc., are defined here.\n",
        "    def update_position(self, position, action):\n",
        "        x, y = position\n",
        "        if action == 0 and y < self.grid_size - 1:\n",
        "            y += 1\n",
        "        elif action == 1 and y > 0:\n",
        "            y -= 1\n",
        "        elif action == 2 and x > 0:\n",
        "            x -= 1\n",
        "        elif action == 3 and x < self.grid_size - 1:\n",
        "            x += 1\n",
        "        return (x, y)\n",
        "\n",
        "    def position_to_state(self, position):\n",
        "        x, y = position\n",
        "        return y * self.grid_size + x + 1\n",
        "\n",
        "    def detect_deception(self, previous_distance, current_distance, goal_position):\n",
        "        # Simple deception detection based on moving away from a goal when very close\n",
        "        if previous_distance == 1 and current_distance > 1:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def step(self, eater_state, action_index):\n",
        "        self.current_mover_state, _ = eater_state\n",
        "        mover_position = self.state_to_position(self.current_mover_state)\n",
        "        self.current_mover_action = self.mover_policy[mover_position]\n",
        "        self.action_history.append(self.current_mover_action)  # Record action\n",
        "        mover_next_position = self.update_position(mover_position, self.current_mover_action)\n",
        "        next_mover_state = self.position_to_state(mover_next_position)\n",
        "\n",
        "        previous_distance_to_g1 = self.calculate_distance(mover_position, self.g1)\n",
        "        previous_distance_to_g2 = self.calculate_distance(mover_position, self.g2)\n",
        "\n",
        "        current_distance_to_g1 = self.calculate_distance(mover_next_position, self.g1)\n",
        "        current_distance_to_g2 = self.calculate_distance(mover_next_position, self.g2)\n",
        "\n",
        "        if self.detect_deception(previous_distance_to_g1, current_distance_to_g1, self.g1):\n",
        "            self.deception_detected = True\n",
        "            self.assumed_true_goal = self.g2\n",
        "        elif self.detect_deception(previous_distance_to_g2, current_distance_to_g2, self.g2):\n",
        "            self.deception_detected = True\n",
        "            self.assumed_true_goal = self.g1\n",
        "\n",
        "        reward = self.calculate_reward(current_distance_to_g1, current_distance_to_g2, action_index)\n",
        "\n",
        "        goal1_consume, goal2_consume = self.actions[action_index]\n",
        "        if self.deception_detected:\n",
        "            action_index = self.maximize_consumption(self.assumed_true_goal)\n",
        "            goal1_consume, goal2_consume = self.actions[action_index]\n",
        "        self.bananas_g1 -= goal1_consume\n",
        "        self.bananas_g2 -= goal2_consume\n",
        "        next_state = (next_mover_state, self.current_mover_action)\n",
        "        if self.current_mover_action == 4:\n",
        "            self.done = True\n",
        "        return next_state, reward, self.done\n",
        "\n",
        "    def maximize_consumption(self, assumed_true_goal):\n",
        "        # Determine which action maximizes consumption from the assumed true goal\n",
        "        if assumed_true_goal == self.g1:\n",
        "            return 0  # Maximum consumption from g1\n",
        "        else:\n",
        "            return 1  # Maximum consumption from g2\n",
        "\n",
        "    def calculate_reward(self, distance_to_g1, distance_to_g2, action_index):\n",
        "        # Extend the reward calculation with deception detection and path prediction\n",
        "        ideal_g1_ratio = distance_to_g2 / (distance_to_g1 + distance_to_g2) if (distance_to_g1 + distance_to_g2) != 0 else 0\n",
        "        ideal_g2_ratio = 1 - ideal_g1_ratio\n",
        "        action_ratios = {\n",
        "            0: (1.0, 0.0), 1: (0.0, 1.0), 2: (0.5, 0.5), 3: (0.2, 0.8), 4: (0.6, 0.4),\n",
        "            5: (0.8, 0.2), 6: (0.4, 0.6), 7: (0.7, 0.3), 8: (0.3, 0.7)\n",
        "        }\n",
        "        actual_g1_ratio, actual_g2_ratio = action_ratios[action_index]\n",
        "        diff_g1 = abs(actual_g1_ratio - ideal_g1_ratio)\n",
        "        diff_g2 = abs(actual_g2_ratio - ideal_g2_ratio)\n",
        "        reward = 1 - (diff_g1 + diff_g2)\n",
        "\n",
        "        # Path prediction and deception detection\n",
        "        predicted_path = self.predict_future_path()\n",
        "        actual_path = self.current_mover_state\n",
        "        return reward\n",
        "\n",
        "    def state_to_position(self, state):\n",
        "        x = (state - 1) % self.grid_size\n",
        "        y = (state - 1) // self.grid_size\n",
        "        return (x, y)\n",
        "\n",
        "    def calculate_distance(self, pos1, pos2):\n",
        "        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
        "\n",
        "    def predict_future_path(self):\n",
        "        # Simple path prediction based on the last observed action\n",
        "        if not self.action_history:\n",
        "            return self.current_mover_state  # No history, return current state\n",
        "        last_action = self.action_history[-1]\n",
        "        predicted_next_position = self.update_position(self.state_to_position(self.current_mover_state), last_action)\n"
      ],
      "metadata": {
        "id": "UWQD_tGlJfxi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mover_policy  = {(0, 0): 0,\n",
        " (0, 1): 0,\n",
        " (0, 2): 0,\n",
        " (0, 3): 0,\n",
        " (0, 4): 0,\n",
        " (0, 5): 3,\n",
        " (0, 6): 1,\n",
        " (1, 0): 2,\n",
        " (1, 1): 1,\n",
        " (1, 2): 2,\n",
        " (1, 3): 2,\n",
        " (1, 4): 0,\n",
        " (1, 5): 3,\n",
        " (1, 6): 1,\n",
        " (2, 0): 2,\n",
        " (2, 1): 0,\n",
        " (2, 2): 0,\n",
        " (2, 3): 2,\n",
        " (2, 4): 0,\n",
        " (2, 5): 3,\n",
        " (2, 6): 2,\n",
        " (3, 0): 0,\n",
        " (3, 1): 0,\n",
        " (3, 2): 0,\n",
        " (3, 3): 0,\n",
        " (3, 4): 0,\n",
        " (3, 5): 0,\n",
        " (3, 6): 3,\n",
        " (4, 0): 2,\n",
        " (4, 1): 0,\n",
        " (4, 2): 0,\n",
        " (4, 3): 2,\n",
        " (4, 4): 2,\n",
        " (4, 5): 0,\n",
        " (4, 6): 3,\n",
        " (5, 0): 2,\n",
        " (5, 1): 1,\n",
        " (5, 2): 3,\n",
        " (5, 3): 3,\n",
        " (5, 4): 0,\n",
        " (5, 5): 0,\n",
        " (5, 6): 3,\n",
        " (6, 0): 1,\n",
        " (6, 1): 1,\n",
        " (6, 2): 3,\n",
        " (6, 3): 0,\n",
        " (6, 4): 0,\n",
        " (6, 5): 2,\n",
        " (6, 6): 4}"
      ],
      "metadata": {
        "id": "MNwQE9JBeZlo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = EaterEnvironment(mover_policy)\n",
        "next_state = (1,0)\n",
        "for i in range(15):\n",
        "  action = random.choice(range(len(env.actions)))\n",
        "  next_state, reward, done = env.step(next_state, action)\n",
        "  print(next_state,action,reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfIklseYuqhm",
        "outputId": "fe1996bc-69a3-4091-d8d1-e64d098877c4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 0) 4 0.825\n",
            "(15, 0) 7 0.9714285714285713\n",
            "(22, 0) 5 0.8999999999999999\n",
            "(29, 0) 0 0.6000000000000001\n",
            "(36, 0) 2 0.25\n",
            "(37, 3) 0 1.0\n",
            "(38, 3) 5 0.9333333333333333\n",
            "(39, 3) 4 0.8666666666666667\n",
            "(46, 0) 7 0.6000000000000001\n",
            "(47, 3) 4 0.46666666666666656\n",
            "(48, 3) 0 -0.6666666666666667\n",
            "(49, 3) 6 0.19999999999999996\n",
            "(49, 4) 3 0.6000000000000001\n",
            "(49, 4) 6 0.19999999999999996\n",
            "(49, 4) 1 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_q_returns_policy(env):\n",
        "    Q = {}\n",
        "    returns = {}\n",
        "    policy = {}\n",
        "    # Assuming there are 49 mover positions and 5 mover actions (0 to 4)\n",
        "    possible_actions = range(len(env.actions))  # Assuming env.actions is defined with 9 possible eater actions\n",
        "\n",
        "    # Initialize Q, returns, and policy for every possible state and action\n",
        "    for mover_position in range(1, 50):  # Mover positions 1 to 49\n",
        "        for mover_action in range(4):  # Mover actions 0 to 4\n",
        "            state = (mover_position, mover_action)\n",
        "            for eater_action in possible_actions:\n",
        "                Q[(state, eater_action)] = 0  # Initial Q-value\n",
        "                returns[(state, eater_action)] = []  # Initialize return list\n",
        "            # Initialize policy with a random action from possible eater actions\n",
        "            policy[state] = random.choice(list(possible_actions))\n",
        "\n",
        "    return Q, returns, policy\n"
      ],
      "metadata": {
        "id": "nrDHXMcXSFyS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def generate_episode(env, policy, epsilon=0.1):\n",
        "    episode = []\n",
        "    env = EaterEnvironment(mover_policy)\n",
        "    current_state = (1, 0)\n",
        "    done = False\n",
        "    while not done:\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(range(len(env.actions)))\n",
        "        else:\n",
        "            action = policy[current_state] if current_state in policy else random.choice(range(len(env.actions)))\n",
        "        next_state, reward, done = env.step(current_state, action)\n",
        "        episode.append((current_state, action, reward))\n",
        "        current_state = next_state\n",
        "    return episode\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "def first_visit_mc(env, num_episodes, epsilon=0.1):\n",
        "    Q, returns, policy = initialize_q_returns_policy(env)\n",
        "\n",
        "    for episode_number in range(num_episodes):\n",
        "        episode = generate_episode(env, policy, epsilon)\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for (state, action, reward) in reversed(episode):\n",
        "            G = reward + 0.99 * G\n",
        "            if (state, action) not in visited:\n",
        "                visited.add((state, action))\n",
        "                returns[(state, action)].append(G)\n",
        "                Q[(state, action)] = np.mean(returns[(state, action)])\n",
        "                best_action = max((Q[(state, a)], a) for a in range(len(env.actions)))[1]\n",
        "                policy[state] = best_action\n",
        "\n",
        "    return policy, Q\n",
        "\n",
        "\n",
        "estimated_policy, Q_values = first_visit_mc(env, 10000, 0.12)\n"
      ],
      "metadata": {
        "id": "Re9537V4SAzs"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(estimated_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2apw7y5rSEwY",
        "outputId": "39ab53e1-267d-4b7e-ee1c-4da45bc8ad4f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(1, 0): 4, (1, 1): 5, (1, 2): 5, (1, 3): 2, (2, 0): 2, (2, 1): 3, (2, 2): 8, (2, 3): 7, (3, 0): 4, (3, 1): 3, (3, 2): 3, (3, 3): 4, (4, 0): 5, (4, 1): 3, (4, 2): 7, (4, 3): 5, (5, 0): 4, (5, 1): 4, (5, 2): 6, (5, 3): 7, (6, 0): 7, (6, 1): 0, (6, 2): 3, (6, 3): 4, (7, 0): 5, (7, 1): 7, (7, 2): 2, (7, 3): 1, (8, 0): 5, (8, 1): 4, (8, 2): 5, (8, 3): 8, (9, 0): 1, (9, 1): 7, (9, 2): 8, (9, 3): 6, (10, 0): 7, (10, 1): 5, (10, 2): 4, (10, 3): 7, (11, 0): 4, (11, 1): 3, (11, 2): 6, (11, 3): 1, (12, 0): 1, (12, 1): 7, (12, 2): 3, (12, 3): 4, (13, 0): 8, (13, 1): 3, (13, 2): 4, (13, 3): 1, (14, 0): 5, (14, 1): 0, (14, 2): 3, (14, 3): 5, (15, 0): 5, (15, 1): 8, (15, 2): 8, (15, 3): 5, (16, 0): 7, (16, 1): 5, (16, 2): 0, (16, 3): 6, (17, 0): 7, (17, 1): 2, (17, 2): 5, (17, 3): 7, (18, 0): 7, (18, 1): 5, (18, 2): 7, (18, 3): 8, (19, 0): 3, (19, 1): 4, (19, 2): 8, (19, 3): 6, (20, 0): 5, (20, 1): 8, (20, 2): 7, (20, 3): 3, (21, 0): 1, (21, 1): 4, (21, 2): 7, (21, 3): 1, (22, 0): 5, (22, 1): 2, (22, 2): 4, (22, 3): 7, (23, 0): 5, (23, 1): 3, (23, 2): 1, (23, 3): 3, (24, 0): 2, (24, 1): 3, (24, 2): 6, (24, 3): 8, (25, 0): 0, (25, 1): 7, (25, 2): 8, (25, 3): 3, (26, 0): 8, (26, 1): 2, (26, 2): 0, (26, 3): 7, (27, 0): 7, (27, 1): 3, (27, 2): 4, (27, 3): 5, (28, 0): 3, (28, 1): 7, (28, 2): 8, (28, 3): 4, (29, 0): 5, (29, 1): 2, (29, 2): 2, (29, 3): 6, (30, 0): 8, (30, 1): 0, (30, 2): 5, (30, 3): 2, (31, 0): 5, (31, 1): 5, (31, 2): 3, (31, 3): 1, (32, 0): 7, (32, 1): 7, (32, 2): 2, (32, 3): 2, (33, 0): 6, (33, 1): 6, (33, 2): 7, (33, 3): 1, (34, 0): 0, (34, 1): 5, (34, 2): 0, (34, 3): 6, (35, 0): 0, (35, 1): 6, (35, 2): 2, (35, 3): 6, (36, 0): 0, (36, 1): 8, (36, 2): 6, (36, 3): 0, (37, 0): 6, (37, 1): 6, (37, 2): 2, (37, 3): 5, (38, 0): 2, (38, 1): 0, (38, 2): 0, (38, 3): 7, (39, 0): 5, (39, 1): 0, (39, 2): 2, (39, 3): 2, (40, 0): 8, (40, 1): 1, (40, 2): 8, (40, 3): 7, (41, 0): 7, (41, 1): 2, (41, 2): 3, (41, 3): 6, (42, 0): 8, (42, 1): 8, (42, 2): 1, (42, 3): 4, (43, 0): 8, (43, 1): 7, (43, 2): 0, (43, 3): 5, (44, 0): 5, (44, 1): 1, (44, 2): 5, (44, 3): 3, (45, 0): 3, (45, 1): 6, (45, 2): 5, (45, 3): 1, (46, 0): 8, (46, 1): 2, (46, 2): 4, (46, 3): 7, (47, 0): 5, (47, 1): 2, (47, 2): 7, (47, 3): 3, (48, 0): 2, (48, 1): 3, (48, 2): 7, (48, 3): 1, (49, 0): 6, (49, 1): 5, (49, 2): 5, (49, 3): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = EaterEnvironment(mover_policy)\n",
        "current_eater_state = (1, 0)  # Initial state\n",
        "\n",
        "for i in range(19):\n",
        "    action_index = estimated_policy[current_eater_state] # Cycle through actions\n",
        "    next_state, reward, done = env.step(current_eater_state, action_index)\n",
        "    if not done:\n",
        "      print(f\"Step {i+1}: Next State: {next_state}, Reward: {reward}, action: {env.actions[action_index]} Bananas at g1: {env.bananas_g1}, Bananas at g2: {env.bananas_g2}\")\n",
        "      current_eater_state = next_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQg3sE-shhSE",
        "outputId": "0d09ca73-4528-4d19-d69c-0727965f3bc1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Next State: (8, 0), Reward: 0.825, action: (0.6, 0.4) Bananas at g1: 99.4, Bananas at g2: 99.6\n",
            "Step 2: Next State: (15, 0), Reward: 0.8285714285714285, action: (0.8, 0.2) Bananas at g1: 98.60000000000001, Bananas at g2: 99.39999999999999\n",
            "Step 3: Next State: (22, 0), Reward: 0.8999999999999999, action: (0.8, 0.2) Bananas at g1: 97.80000000000001, Bananas at g2: 99.19999999999999\n",
            "Step 4: Next State: (29, 0), Reward: 1.0, action: (0.8, 0.2) Bananas at g1: 97.00000000000001, Bananas at g2: 98.99999999999999\n",
            "Step 5: Next State: (36, 0), Reward: 0.8500000000000001, action: (0.8, 0.2) Bananas at g1: 96.20000000000002, Bananas at g2: 98.79999999999998\n",
            "Step 6: Next State: (37, 3), Reward: 1.0, action: (1, 0) Bananas at g1: 95.20000000000002, Bananas at g2: 98.79999999999998\n",
            "Step 7: Next State: (38, 3), Reward: 0.9333333333333333, action: (0.8, 0.2) Bananas at g1: 94.40000000000002, Bananas at g2: 98.59999999999998\n",
            "Step 8: Next State: (39, 3), Reward: 0.9333333333333333, action: (0.7, 0.3) Bananas at g1: 94.40000000000002, Bananas at g2: 97.59999999999998\n",
            "Step 9: Next State: (46, 0), Reward: 1.0, action: (0.5, 0.5) Bananas at g1: 94.40000000000002, Bananas at g2: 96.59999999999998\n",
            "Step 10: Next State: (47, 3), Reward: 0.9333333333333335, action: (0.3, 0.7) Bananas at g1: 94.40000000000002, Bananas at g2: 95.59999999999998\n",
            "Step 11: Next State: (48, 3), Reward: 0.9333333333333333, action: (0.2, 0.8) Bananas at g1: 94.40000000000002, Bananas at g2: 94.59999999999998\n",
            "Step 12: Next State: (49, 3), Reward: 1.0, action: (0, 1) Bananas at g1: 94.40000000000002, Bananas at g2: 93.59999999999998\n"
          ]
        }
      ]
    }
  ]
}