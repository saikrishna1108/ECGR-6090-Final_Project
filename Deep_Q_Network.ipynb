{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class EaterEnvironment:\n",
        "    def __init__(self, mover_policy, grid_size=7, g1_state=37, g2_state=49, initial_bananas_g1=100, initial_bananas_g2=100):\n",
        "        self.mover_policy = mover_policy\n",
        "        self.grid_size = grid_size\n",
        "        self.g1 = self.state_to_position(g1_state)\n",
        "        self.g2 = self.state_to_position(g2_state)\n",
        "        self.bananas_g1 = initial_bananas_g1\n",
        "        self.bananas_g2 = initial_bananas_g2\n",
        "        self.actions = [\n",
        "            (1, 0), (0, 1), (0.5, 0.5), (0.2, 0.8), (0.6, 0.4),\n",
        "            (0.8, 0.2), (0.4, 0.6), (0.7, 0.3), (0.3, 0.7)\n",
        "        ]\n",
        "        self.initial_mover_state = 1\n",
        "        self.current_mover_state = self.initial_mover_state\n",
        "        self.current_mover_action = 0\n",
        "        self.done = False\n",
        "        self.action_history = []\n",
        "        self.min_distance_to_g1 = float('inf')\n",
        "        self.min_distance_to_g2 = float('inf')\n",
        "        self.deception_detected = False\n",
        "        self.assumed_true_goal = None  # Assumed true goal after deception is detected\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_mover_state = 1\n",
        "        self.current_mover_action = random.choice(range(len(self.actions)))\n",
        "        self.done = False\n",
        "        self.action_history.clear()\n",
        "        self.min_distance_to_g1 = float('inf')\n",
        "        self.min_distance_to_g2 = float('inf')\n",
        "        self.deception_detected = False\n",
        "        self.assumed_true_goal = None\n",
        "        return (self.current_mover_state, self.current_mover_action)\n",
        "\n",
        "    # Assume other methods like state_to_position, calculate_distance, etc., are defined here.\n",
        "    def update_position(self, position, action):\n",
        "        x, y = position\n",
        "        if action == 0 and y < self.grid_size - 1:\n",
        "            y += 1\n",
        "        elif action == 1 and y > 0:\n",
        "            y -= 1\n",
        "        elif action == 2 and x > 0:\n",
        "            x -= 1\n",
        "        elif action == 3 and x < self.grid_size - 1:\n",
        "            x += 1\n",
        "        return (x, y)\n",
        "\n",
        "    def position_to_state(self, position):\n",
        "        x, y = position\n",
        "        return y * self.grid_size + x + 1\n",
        "\n",
        "    def detect_deception(self, previous_distance, current_distance, goal_position):\n",
        "        # Simple deception detection based on moving away from a goal when very close\n",
        "        if previous_distance < 2 and current_distance > 1:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def step(self, eater_state, action_index):\n",
        "        self.current_mover_state, _ = eater_state\n",
        "        mover_position = self.state_to_position(self.current_mover_state)\n",
        "        self.current_mover_action = self.mover_policy[mover_position]\n",
        "        self.action_history.append(self.current_mover_action)  # Record action\n",
        "        mover_next_position = self.update_position(mover_position, self.current_mover_action)\n",
        "        next_mover_state = self.position_to_state(mover_next_position)\n",
        "\n",
        "        previous_distance_to_g1 = self.calculate_distance(mover_position, self.g1)\n",
        "        previous_distance_to_g2 = self.calculate_distance(mover_position, self.g2)\n",
        "\n",
        "        current_distance_to_g1 = self.calculate_distance(mover_next_position, self.g1)\n",
        "        current_distance_to_g2 = self.calculate_distance(mover_next_position, self.g2)\n",
        "\n",
        "        if self.detect_deception(previous_distance_to_g1, current_distance_to_g1, self.g1):\n",
        "            self.deception_detected = True\n",
        "            self.assumed_true_goal = self.g2\n",
        "        elif self.detect_deception(previous_distance_to_g2, current_distance_to_g2, self.g2):\n",
        "            self.deception_detected = True\n",
        "            self.assumed_true_goal = self.g1\n",
        "\n",
        "        reward = self.calculate_reward(current_distance_to_g1, current_distance_to_g2, action_index)\n",
        "\n",
        "        goal1_consume, goal2_consume = self.actions[action_index]\n",
        "        if self.deception_detected:\n",
        "            action_index = self.maximize_consumption(self.assumed_true_goal)\n",
        "            goal1_consume, goal2_consume = self.actions[action_index]\n",
        "        self.bananas_g1 -= goal1_consume\n",
        "        self.bananas_g2 -= goal2_consume\n",
        "        next_state = (next_mover_state, self.current_mover_action)\n",
        "        if self.current_mover_action == 4:\n",
        "            self.done = True\n",
        "        return next_state, reward, self.done\n",
        "\n",
        "    def maximize_consumption(self, assumed_true_goal):\n",
        "        # Determine which action maximizes consumption from the assumed true goal\n",
        "        if assumed_true_goal == self.g1:\n",
        "            return 0  # Maximum consumption from g1\n",
        "        else:\n",
        "            return 1  # Maximum consumption from g2\n",
        "\n",
        "    def calculate_reward(self, distance_to_g1, distance_to_g2, action_index):\n",
        "        # Extend the reward calculation with deception detection and path prediction\n",
        "        ideal_g1_ratio = distance_to_g2 / (distance_to_g1 + distance_to_g2) if (distance_to_g1 + distance_to_g2) != 0 else 0\n",
        "        ideal_g2_ratio = 1 - ideal_g1_ratio\n",
        "        action_ratios = {\n",
        "            0: (1.0, 0.0), 1: (0.0, 1.0), 2: (0.5, 0.5), 3: (0.2, 0.8), 4: (0.6, 0.4),\n",
        "            5: (0.8, 0.2), 6: (0.4, 0.6), 7: (0.7, 0.3), 8: (0.3, 0.7)\n",
        "        }\n",
        "        actual_g1_ratio, actual_g2_ratio = action_ratios[action_index]\n",
        "        diff_g1 = abs(actual_g1_ratio - ideal_g1_ratio)\n",
        "        diff_g2 = abs(actual_g2_ratio - ideal_g2_ratio)\n",
        "        reward = 1 - (diff_g1 + diff_g2)\n",
        "\n",
        "        # Path prediction and deception detection\n",
        "        predicted_path = self.predict_future_path()\n",
        "        actual_path = self.current_mover_state\n",
        "        return reward\n",
        "\n",
        "    def state_to_position(self, state):\n",
        "        x = (state - 1) % self.grid_size\n",
        "        y = (state - 1) // self.grid_size\n",
        "        return (x, y)\n",
        "\n",
        "    def calculate_distance(self, pos1, pos2):\n",
        "        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
        "\n",
        "    def predict_future_path(self):\n",
        "        # Simple path prediction based on the last observed action\n",
        "        if not self.action_history:\n",
        "            return self.current_mover_state  # No history, return current state\n",
        "        last_action = self.action_history[-1]\n",
        "        predicted_next_position = self.update_position(self.state_to_position(self.current_mover_state), last_action)\n"
      ],
      "metadata": {
        "id": "CjumrbWINTE_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mover_policy  = {(0, 0): 0,\n",
        " (0, 1): 0,\n",
        " (0, 2): 0,\n",
        " (0, 3): 0,\n",
        " (0, 4): 0,\n",
        " (0, 5): 3,\n",
        " (0, 6): 1,\n",
        " (1, 0): 2,\n",
        " (1, 1): 1,\n",
        " (1, 2): 2,\n",
        " (1, 3): 2,\n",
        " (1, 4): 0,\n",
        " (1, 5): 3,\n",
        " (1, 6): 1,\n",
        " (2, 0): 2,\n",
        " (2, 1): 0,\n",
        " (2, 2): 0,\n",
        " (2, 3): 2,\n",
        " (2, 4): 0,\n",
        " (2, 5): 3,\n",
        " (2, 6): 2,\n",
        " (3, 0): 0,\n",
        " (3, 1): 0,\n",
        " (3, 2): 0,\n",
        " (3, 3): 0,\n",
        " (3, 4): 0,\n",
        " (3, 5): 0,\n",
        " (3, 6): 3,\n",
        " (4, 0): 2,\n",
        " (4, 1): 0,\n",
        " (4, 2): 0,\n",
        " (4, 3): 2,\n",
        " (4, 4): 2,\n",
        " (4, 5): 0,\n",
        " (4, 6): 3,\n",
        " (5, 0): 2,\n",
        " (5, 1): 1,\n",
        " (5, 2): 3,\n",
        " (5, 3): 3,\n",
        " (5, 4): 0,\n",
        " (5, 5): 0,\n",
        " (5, 6): 3,\n",
        " (6, 0): 1,\n",
        " (6, 1): 1,\n",
        " (6, 2): 3,\n",
        " (6, 3): 0,\n",
        " (6, 4): 0,\n",
        " (6, 5): 2,\n",
        " (6, 6): 4}"
      ],
      "metadata": {
        "id": "Xmxwfb1HQEYF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "env = EaterEnvironment(mover_policy)\n",
        "next_state = (1,0)\n",
        "for i in range(10):\n",
        "  action = random.choice(range(len(env.actions)))\n",
        "  next_state, reward, done = env.step(next_state, action)\n",
        "  if done == True:\n",
        "    break\n",
        "  print(next_state, reward, done)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0KZ805wQFKE",
        "outputId": "6e7d4e23-e411-4d3f-f07f-9124466171e5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 0) 0.7749999999999999 False\n",
            "(15, 0) 0.8285714285714285 False\n",
            "(22, 0) -0.10000000000000009 False\n",
            "(29, 0) 1.0 False\n",
            "(36, 0) 0.25 False\n",
            "(37, 3) -0.19999999999999996 False\n",
            "(38, 3) 0.33333333333333326 False\n",
            "(39, 3) 0.6666666666666667 False\n",
            "(46, 0) 0.6000000000000001 False\n",
            "(47, 3) 0.46666666666666656 False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Define the neural network model\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 16)\n",
        "        self.fc2 = nn.Linear(16, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "kxf0qSZMQigU"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming QNetwork and ReplayBuffer are defined above\n",
        "\n",
        "state_dim = 2  # Define the size of the input state dimensions\n",
        "action_dim = len(env.actions)  # Define the number of actions available in the environment\n",
        "buffer_capacity = 10000\n",
        "batch_size = 10\n",
        "model = QNetwork(state_dim, action_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "total_episodes = 5000\n",
        "\n",
        "def compute_loss(batch):\n",
        "    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions)\n",
        "    rewards = torch.tensor(rewards)\n",
        "    dones = torch.tensor(dones)\n",
        "\n",
        "    current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "    next_q_values = model(next_states).max(1)[0]\n",
        "    next_q_values[dones] = 0.0  # Zero out the values for terminal states\n",
        "    target_q_values = rewards + gamma * next_q_values\n",
        "\n",
        "    loss = nn.MSELoss()(current_q_values, target_q_values)\n",
        "    return loss\n",
        "\n",
        "for epoch in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        q_values = model(state_tensor)\n",
        "        action = torch.argmax(q_values).item() if random.random() > epsilon else random.randint(0, action_dim - 1)\n",
        "        next_state, reward, done = env.step(state, action)\n",
        "\n",
        "        replay_buffer.push(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "\n",
        "        # Regular update if enough samples are collected\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            batch = replay_buffer.sample(batch_size)\n",
        "            loss = compute_loss(batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Update at the end of the episode using all available samples if fewer than batch_size\n",
        "    if done and len(replay_buffer) < batch_size:\n",
        "        batch = replay_buffer.sample(len(replay_buffer))  # Take all that is available\n",
        "        loss = compute_loss(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch:\", epoch + 1, \"Loss:\", loss.item() if 'loss' in locals() else 'No update')\n",
        "print(\"Final loss:\", loss.item() if 'loss' in locals() else 'No final update')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lwUr4YDQlSK",
        "outputId": "e3fd927b-372f-4073-c0f5-558c058ffb61"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 4.440133094787598\n",
            "Epoch: 11 Loss: 0.4337710738182068\n",
            "Epoch: 21 Loss: 0.26174014806747437\n",
            "Epoch: 31 Loss: 0.05129067972302437\n",
            "Epoch: 41 Loss: 0.1471439152956009\n",
            "Epoch: 51 Loss: 0.15136206150054932\n",
            "Epoch: 61 Loss: 0.2854503393173218\n",
            "Epoch: 71 Loss: 0.12240271270275116\n",
            "Epoch: 81 Loss: 0.09675641357898712\n",
            "Epoch: 91 Loss: 0.14667868614196777\n",
            "Epoch: 101 Loss: 0.10476432740688324\n",
            "Epoch: 111 Loss: 0.1501254141330719\n",
            "Epoch: 121 Loss: 0.21464891731739044\n",
            "Epoch: 131 Loss: 0.13221785426139832\n",
            "Epoch: 141 Loss: 0.20359483361244202\n",
            "Epoch: 151 Loss: 0.07239727675914764\n",
            "Epoch: 161 Loss: 0.2690611779689789\n",
            "Epoch: 171 Loss: 0.07715148478746414\n",
            "Epoch: 181 Loss: 0.017327217385172844\n",
            "Epoch: 191 Loss: 0.15159226953983307\n",
            "Epoch: 201 Loss: 0.12241852283477783\n",
            "Epoch: 211 Loss: 0.10833646357059479\n",
            "Epoch: 221 Loss: 0.2009856253862381\n",
            "Epoch: 231 Loss: 0.1792593151330948\n",
            "Epoch: 241 Loss: 0.10419623553752899\n",
            "Epoch: 251 Loss: 0.04052206873893738\n",
            "Epoch: 261 Loss: 0.1610371470451355\n",
            "Epoch: 271 Loss: 0.1810162514448166\n",
            "Epoch: 281 Loss: 0.05696675926446915\n",
            "Epoch: 291 Loss: 0.06057814881205559\n",
            "Epoch: 301 Loss: 0.1551976203918457\n",
            "Epoch: 311 Loss: 0.12472347170114517\n",
            "Epoch: 321 Loss: 0.14582660794258118\n",
            "Epoch: 331 Loss: 0.0367344506084919\n",
            "Epoch: 341 Loss: 0.10075946152210236\n",
            "Epoch: 351 Loss: 0.041168272495269775\n",
            "Epoch: 361 Loss: 0.08823967725038528\n",
            "Epoch: 371 Loss: 0.11120031028985977\n",
            "Epoch: 381 Loss: 0.09858818352222443\n",
            "Epoch: 391 Loss: 0.20575428009033203\n",
            "Epoch: 401 Loss: 0.15503685176372528\n",
            "Epoch: 411 Loss: 0.14407750964164734\n",
            "Epoch: 421 Loss: 0.11460087448358536\n",
            "Epoch: 431 Loss: 0.0831613689661026\n",
            "Epoch: 441 Loss: 0.09400145709514618\n",
            "Epoch: 451 Loss: 0.06695046275854111\n",
            "Epoch: 461 Loss: 0.11366836726665497\n",
            "Epoch: 471 Loss: 0.05974448844790459\n",
            "Epoch: 481 Loss: 0.1761532872915268\n",
            "Epoch: 491 Loss: 0.07275910675525665\n",
            "Epoch: 501 Loss: 0.19696451723575592\n",
            "Epoch: 511 Loss: 0.0882965624332428\n",
            "Epoch: 521 Loss: 0.18758796155452728\n",
            "Epoch: 531 Loss: 0.04171903431415558\n",
            "Epoch: 541 Loss: 0.09609495103359222\n",
            "Epoch: 551 Loss: 0.18021973967552185\n",
            "Epoch: 561 Loss: 0.19954003393650055\n",
            "Epoch: 571 Loss: 0.04283086583018303\n",
            "Epoch: 581 Loss: 0.06456026434898376\n",
            "Epoch: 591 Loss: 0.10386599600315094\n",
            "Epoch: 601 Loss: 0.0744825080037117\n",
            "Epoch: 611 Loss: 0.08757840096950531\n",
            "Epoch: 621 Loss: 0.19994661211967468\n",
            "Epoch: 631 Loss: 0.06845174729824066\n",
            "Epoch: 641 Loss: 0.13579395413398743\n",
            "Epoch: 651 Loss: 0.360726535320282\n",
            "Epoch: 661 Loss: 0.14275887608528137\n",
            "Epoch: 671 Loss: 0.1883731633424759\n",
            "Epoch: 681 Loss: 0.1330181211233139\n",
            "Epoch: 691 Loss: 0.10641815513372421\n",
            "Epoch: 701 Loss: 0.09762410819530487\n",
            "Epoch: 711 Loss: 0.06649019569158554\n",
            "Epoch: 721 Loss: 0.04988693445920944\n",
            "Epoch: 731 Loss: 0.12780627608299255\n",
            "Epoch: 741 Loss: 0.041810594499111176\n",
            "Epoch: 751 Loss: 0.11323440074920654\n",
            "Epoch: 761 Loss: 0.09052455425262451\n",
            "Epoch: 771 Loss: 0.052557479590177536\n",
            "Epoch: 781 Loss: 0.03696797043085098\n",
            "Epoch: 791 Loss: 0.050435375422239304\n",
            "Epoch: 801 Loss: 0.10228314250707626\n",
            "Epoch: 811 Loss: 0.12522146105766296\n",
            "Epoch: 821 Loss: 0.19758039712905884\n",
            "Epoch: 831 Loss: 0.13868117332458496\n",
            "Epoch: 841 Loss: 0.03603944927453995\n",
            "Epoch: 851 Loss: 0.12073059380054474\n",
            "Epoch: 861 Loss: 0.13035497069358826\n",
            "Epoch: 871 Loss: 0.02522646263241768\n",
            "Epoch: 881 Loss: 0.0450858399271965\n",
            "Epoch: 891 Loss: 0.060324352234601974\n",
            "Epoch: 901 Loss: 0.042929984629154205\n",
            "Epoch: 911 Loss: 0.10516810417175293\n",
            "Epoch: 921 Loss: 0.09185628592967987\n",
            "Epoch: 931 Loss: 0.04967150092124939\n",
            "Epoch: 941 Loss: 0.06830418109893799\n",
            "Epoch: 951 Loss: 0.07503440976142883\n",
            "Epoch: 961 Loss: 0.08492503315210342\n",
            "Epoch: 971 Loss: 0.00610940670594573\n",
            "Epoch: 981 Loss: 0.08629297465085983\n",
            "Epoch: 991 Loss: 0.08211956918239594\n",
            "Epoch: 1001 Loss: 0.1485302746295929\n",
            "Epoch: 1011 Loss: 0.2041911780834198\n",
            "Epoch: 1021 Loss: 0.044591933488845825\n",
            "Epoch: 1031 Loss: 0.08306755870580673\n",
            "Epoch: 1041 Loss: 0.013055764138698578\n",
            "Epoch: 1051 Loss: 0.07562999427318573\n",
            "Epoch: 1061 Loss: 0.09840415418148041\n",
            "Epoch: 1071 Loss: 0.16658946871757507\n",
            "Epoch: 1081 Loss: 0.0775725394487381\n",
            "Epoch: 1091 Loss: 0.13757851719856262\n",
            "Epoch: 1101 Loss: 0.12423133850097656\n",
            "Epoch: 1111 Loss: 0.10557396709918976\n",
            "Epoch: 1121 Loss: 0.06881895661354065\n",
            "Epoch: 1131 Loss: 0.03399980813264847\n",
            "Epoch: 1141 Loss: 0.10797147452831268\n",
            "Epoch: 1151 Loss: 0.15526144206523895\n",
            "Epoch: 1161 Loss: 0.024448800832033157\n",
            "Epoch: 1171 Loss: 0.14353212714195251\n",
            "Epoch: 1181 Loss: 0.08391822874546051\n",
            "Epoch: 1191 Loss: 0.07246855646371841\n",
            "Epoch: 1201 Loss: 0.08612902462482452\n",
            "Epoch: 1211 Loss: 0.14261014759540558\n",
            "Epoch: 1221 Loss: 0.10142223536968231\n",
            "Epoch: 1231 Loss: 0.10208576917648315\n",
            "Epoch: 1241 Loss: 0.10657554864883423\n",
            "Epoch: 1251 Loss: 0.11203155666589737\n",
            "Epoch: 1261 Loss: 0.05470975488424301\n",
            "Epoch: 1271 Loss: 0.11730657517910004\n",
            "Epoch: 1281 Loss: 0.1083030104637146\n",
            "Epoch: 1291 Loss: 0.07113179564476013\n",
            "Epoch: 1301 Loss: 0.07662815600633621\n",
            "Epoch: 1311 Loss: 0.07374779880046844\n",
            "Epoch: 1321 Loss: 0.049881696701049805\n",
            "Epoch: 1331 Loss: 0.08284015953540802\n",
            "Epoch: 1341 Loss: 0.14242002367973328\n",
            "Epoch: 1351 Loss: 0.1700364500284195\n",
            "Epoch: 1361 Loss: 0.08428643643856049\n",
            "Epoch: 1371 Loss: 0.26367220282554626\n",
            "Epoch: 1381 Loss: 0.043144047260284424\n",
            "Epoch: 1391 Loss: 0.14009703695774078\n",
            "Epoch: 1401 Loss: 0.18582388758659363\n",
            "Epoch: 1411 Loss: 0.1222003921866417\n",
            "Epoch: 1421 Loss: 0.1709337681531906\n",
            "Epoch: 1431 Loss: 0.11406806856393814\n",
            "Epoch: 1441 Loss: 0.23842331767082214\n",
            "Epoch: 1451 Loss: 0.08953619003295898\n",
            "Epoch: 1461 Loss: 0.05876947194337845\n",
            "Epoch: 1471 Loss: 0.07739213109016418\n",
            "Epoch: 1481 Loss: 0.16383643448352814\n",
            "Epoch: 1491 Loss: 0.1194072961807251\n",
            "Epoch: 1501 Loss: 0.11370149999856949\n",
            "Epoch: 1511 Loss: 0.18927165865898132\n",
            "Epoch: 1521 Loss: 0.11550469696521759\n",
            "Epoch: 1531 Loss: 0.10643406212329865\n",
            "Epoch: 1541 Loss: 0.2387319803237915\n",
            "Epoch: 1551 Loss: 0.11655459553003311\n",
            "Epoch: 1561 Loss: 0.05119261145591736\n",
            "Epoch: 1571 Loss: 0.0624026358127594\n",
            "Epoch: 1581 Loss: 0.09000755101442337\n",
            "Epoch: 1591 Loss: 0.16049787402153015\n",
            "Epoch: 1601 Loss: 0.12283281236886978\n",
            "Epoch: 1611 Loss: 0.09206078946590424\n",
            "Epoch: 1621 Loss: 0.1034349575638771\n",
            "Epoch: 1631 Loss: 0.07475067675113678\n",
            "Epoch: 1641 Loss: 0.13907797634601593\n",
            "Epoch: 1651 Loss: 0.07898211479187012\n",
            "Epoch: 1661 Loss: 0.04800943657755852\n",
            "Epoch: 1671 Loss: 0.10892145335674286\n",
            "Epoch: 1681 Loss: 0.07058088481426239\n",
            "Epoch: 1691 Loss: 0.05971624329686165\n",
            "Epoch: 1701 Loss: 0.11497598886489868\n",
            "Epoch: 1711 Loss: 0.1058417409658432\n",
            "Epoch: 1721 Loss: 0.10672636330127716\n",
            "Epoch: 1731 Loss: 0.02753078006207943\n",
            "Epoch: 1741 Loss: 0.1546299159526825\n",
            "Epoch: 1751 Loss: 0.06097409874200821\n",
            "Epoch: 1761 Loss: 0.0827365517616272\n",
            "Epoch: 1771 Loss: 0.029585039243102074\n",
            "Epoch: 1781 Loss: 0.13547278940677643\n",
            "Epoch: 1791 Loss: 0.037973687052726746\n",
            "Epoch: 1801 Loss: 0.08851142227649689\n",
            "Epoch: 1811 Loss: 0.19041942059993744\n",
            "Epoch: 1821 Loss: 0.11202812194824219\n",
            "Epoch: 1831 Loss: 0.07699424773454666\n",
            "Epoch: 1841 Loss: 0.1704637110233307\n",
            "Epoch: 1851 Loss: 0.24724721908569336\n",
            "Epoch: 1861 Loss: 0.011354388669133186\n",
            "Epoch: 1871 Loss: 0.08376462012529373\n",
            "Epoch: 1881 Loss: 0.10461854934692383\n",
            "Epoch: 1891 Loss: 0.12797820568084717\n",
            "Epoch: 1901 Loss: 0.015021217986941338\n",
            "Epoch: 1911 Loss: 0.09814028441905975\n",
            "Epoch: 1921 Loss: 0.08347456157207489\n",
            "Epoch: 1931 Loss: 0.10383477061986923\n",
            "Epoch: 1941 Loss: 0.1158570647239685\n",
            "Epoch: 1951 Loss: 0.06086983531713486\n",
            "Epoch: 1961 Loss: 0.11782810837030411\n",
            "Epoch: 1971 Loss: 0.14301006495952606\n",
            "Epoch: 1981 Loss: 0.1304502785205841\n",
            "Epoch: 1991 Loss: 0.1110910177230835\n",
            "Epoch: 2001 Loss: 0.08817503601312637\n",
            "Epoch: 2011 Loss: 0.11666630208492279\n",
            "Epoch: 2021 Loss: 0.07618364691734314\n",
            "Epoch: 2031 Loss: 0.0833258181810379\n",
            "Epoch: 2041 Loss: 0.08880212157964706\n",
            "Epoch: 2051 Loss: 0.010114242322742939\n",
            "Epoch: 2061 Loss: 0.040943436324596405\n",
            "Epoch: 2071 Loss: 0.1100936084985733\n",
            "Epoch: 2081 Loss: 0.03509116917848587\n",
            "Epoch: 2091 Loss: 0.10728245973587036\n",
            "Epoch: 2101 Loss: 0.14374801516532898\n",
            "Epoch: 2111 Loss: 0.11793527752161026\n",
            "Epoch: 2121 Loss: 0.10697730630636215\n",
            "Epoch: 2131 Loss: 0.20591607689857483\n",
            "Epoch: 2141 Loss: 0.038638584315776825\n",
            "Epoch: 2151 Loss: 0.3288993835449219\n",
            "Epoch: 2161 Loss: 0.09240638464689255\n",
            "Epoch: 2171 Loss: 0.12370803207159042\n",
            "Epoch: 2181 Loss: 0.04301520064473152\n",
            "Epoch: 2191 Loss: 0.024118004366755486\n",
            "Epoch: 2201 Loss: 0.08037233352661133\n",
            "Epoch: 2211 Loss: 0.11952481418848038\n",
            "Epoch: 2221 Loss: 0.037752993404865265\n",
            "Epoch: 2231 Loss: 0.06852675974369049\n",
            "Epoch: 2241 Loss: 0.08228915184736252\n",
            "Epoch: 2251 Loss: 0.023874571546912193\n",
            "Epoch: 2261 Loss: 0.04790932312607765\n",
            "Epoch: 2271 Loss: 0.08414305746555328\n",
            "Epoch: 2281 Loss: 0.0657917708158493\n",
            "Epoch: 2291 Loss: 0.09120192378759384\n",
            "Epoch: 2301 Loss: 0.057730190455913544\n",
            "Epoch: 2311 Loss: 0.10021717846393585\n",
            "Epoch: 2321 Loss: 0.08385059237480164\n",
            "Epoch: 2331 Loss: 0.04754943400621414\n",
            "Epoch: 2341 Loss: 0.05023274943232536\n",
            "Epoch: 2351 Loss: 0.10034000873565674\n",
            "Epoch: 2361 Loss: 0.11359582096338272\n",
            "Epoch: 2371 Loss: 0.0995030552148819\n",
            "Epoch: 2381 Loss: 0.17245836555957794\n",
            "Epoch: 2391 Loss: 0.03347635269165039\n",
            "Epoch: 2401 Loss: 0.04442770406603813\n",
            "Epoch: 2411 Loss: 0.06007780879735947\n",
            "Epoch: 2421 Loss: 0.21443995833396912\n",
            "Epoch: 2431 Loss: 0.17903560400009155\n",
            "Epoch: 2441 Loss: 0.12920799851417542\n",
            "Epoch: 2451 Loss: 0.06423633545637131\n",
            "Epoch: 2461 Loss: 0.15318140387535095\n",
            "Epoch: 2471 Loss: 0.07693691551685333\n",
            "Epoch: 2481 Loss: 0.1798887699842453\n",
            "Epoch: 2491 Loss: 0.15730299055576324\n",
            "Epoch: 2501 Loss: 0.1233299970626831\n",
            "Epoch: 2511 Loss: 0.04110328108072281\n",
            "Epoch: 2521 Loss: 0.15494577586650848\n",
            "Epoch: 2531 Loss: 0.15607674419879913\n",
            "Epoch: 2541 Loss: 0.13073663413524628\n",
            "Epoch: 2551 Loss: 0.09086211770772934\n",
            "Epoch: 2561 Loss: 0.03422725200653076\n",
            "Epoch: 2571 Loss: 0.25641363859176636\n",
            "Epoch: 2581 Loss: 0.06811363995075226\n",
            "Epoch: 2591 Loss: 0.10466054826974869\n",
            "Epoch: 2601 Loss: 0.10914714634418488\n",
            "Epoch: 2611 Loss: 0.16607126593589783\n",
            "Epoch: 2621 Loss: 0.0603923499584198\n",
            "Epoch: 2631 Loss: 0.13206544518470764\n",
            "Epoch: 2641 Loss: 0.13563722372055054\n",
            "Epoch: 2651 Loss: 0.1650516241788864\n",
            "Epoch: 2661 Loss: 0.14097380638122559\n",
            "Epoch: 2671 Loss: 0.119534432888031\n",
            "Epoch: 2681 Loss: 0.08316822350025177\n",
            "Epoch: 2691 Loss: 0.12246398627758026\n",
            "Epoch: 2701 Loss: 0.12611767649650574\n",
            "Epoch: 2711 Loss: 0.03904600068926811\n",
            "Epoch: 2721 Loss: 0.1662842035293579\n",
            "Epoch: 2731 Loss: 0.1694551557302475\n",
            "Epoch: 2741 Loss: 0.06734826415777206\n",
            "Epoch: 2751 Loss: 0.11233489215373993\n",
            "Epoch: 2761 Loss: 0.1582922339439392\n",
            "Epoch: 2771 Loss: 0.09381209313869476\n",
            "Epoch: 2781 Loss: 0.15275157988071442\n",
            "Epoch: 2791 Loss: 0.05271108075976372\n",
            "Epoch: 2801 Loss: 0.11308439075946808\n",
            "Epoch: 2811 Loss: 0.1949159801006317\n",
            "Epoch: 2821 Loss: 0.10028345882892609\n",
            "Epoch: 2831 Loss: 0.07973481714725494\n",
            "Epoch: 2841 Loss: 0.16430971026420593\n",
            "Epoch: 2851 Loss: 0.11427094042301178\n",
            "Epoch: 2861 Loss: 0.15815402567386627\n",
            "Epoch: 2871 Loss: 0.06179634481668472\n",
            "Epoch: 2881 Loss: 0.07956866174936295\n",
            "Epoch: 2891 Loss: 0.12653328478336334\n",
            "Epoch: 2901 Loss: 0.1027187928557396\n",
            "Epoch: 2911 Loss: 0.10225661098957062\n",
            "Epoch: 2921 Loss: 0.28682178258895874\n",
            "Epoch: 2931 Loss: 0.038260318338871\n",
            "Epoch: 2941 Loss: 0.19728663563728333\n",
            "Epoch: 2951 Loss: 0.12961982190608978\n",
            "Epoch: 2961 Loss: 0.18870319426059723\n",
            "Epoch: 2971 Loss: 0.15999792516231537\n",
            "Epoch: 2981 Loss: 0.150871142745018\n",
            "Epoch: 2991 Loss: 0.05299790948629379\n",
            "Epoch: 3001 Loss: 0.08577422797679901\n",
            "Epoch: 3011 Loss: 0.11076784133911133\n",
            "Epoch: 3021 Loss: 0.11967416852712631\n",
            "Epoch: 3031 Loss: 0.06868632137775421\n",
            "Epoch: 3041 Loss: 0.12546247243881226\n",
            "Epoch: 3051 Loss: 0.07473529130220413\n",
            "Epoch: 3061 Loss: 0.13491806387901306\n",
            "Epoch: 3071 Loss: 0.1029185801744461\n",
            "Epoch: 3081 Loss: 0.14176467061042786\n",
            "Epoch: 3091 Loss: 0.08192677795886993\n",
            "Epoch: 3101 Loss: 0.07404901087284088\n",
            "Epoch: 3111 Loss: 0.11608032137155533\n",
            "Epoch: 3121 Loss: 0.10409977287054062\n",
            "Epoch: 3131 Loss: 0.23102831840515137\n",
            "Epoch: 3141 Loss: 0.0798993855714798\n",
            "Epoch: 3151 Loss: 0.10508565604686737\n",
            "Epoch: 3161 Loss: 0.02892995811998844\n",
            "Epoch: 3171 Loss: 0.07379251718521118\n",
            "Epoch: 3181 Loss: 0.1366065889596939\n",
            "Epoch: 3191 Loss: 0.06136080622673035\n",
            "Epoch: 3201 Loss: 0.07776442170143127\n",
            "Epoch: 3211 Loss: 0.12259887158870697\n",
            "Epoch: 3221 Loss: 0.23342935740947723\n",
            "Epoch: 3231 Loss: 0.13971462845802307\n",
            "Epoch: 3241 Loss: 0.1678769886493683\n",
            "Epoch: 3251 Loss: 0.08433336764574051\n",
            "Epoch: 3261 Loss: 0.25331392884254456\n",
            "Epoch: 3271 Loss: 0.05055570602416992\n",
            "Epoch: 3281 Loss: 0.06290894001722336\n",
            "Epoch: 3291 Loss: 0.05282914638519287\n",
            "Epoch: 3301 Loss: 0.08016200363636017\n",
            "Epoch: 3311 Loss: 0.1603153496980667\n",
            "Epoch: 3321 Loss: 0.15674714744091034\n",
            "Epoch: 3331 Loss: 0.15362803637981415\n",
            "Epoch: 3341 Loss: 0.0677788183093071\n",
            "Epoch: 3351 Loss: 0.08532027900218964\n",
            "Epoch: 3361 Loss: 0.1343790590763092\n",
            "Epoch: 3371 Loss: 0.06721142679452896\n",
            "Epoch: 3381 Loss: 0.07506013661623001\n",
            "Epoch: 3391 Loss: 0.03563819080591202\n",
            "Epoch: 3401 Loss: 0.06784096360206604\n",
            "Epoch: 3411 Loss: 0.07151855528354645\n",
            "Epoch: 3421 Loss: 0.14642181992530823\n",
            "Epoch: 3431 Loss: 0.030225951224565506\n",
            "Epoch: 3441 Loss: 0.0763002410531044\n",
            "Epoch: 3451 Loss: 0.1427091658115387\n",
            "Epoch: 3461 Loss: 0.022382473573088646\n",
            "Epoch: 3471 Loss: 0.1484450101852417\n",
            "Epoch: 3481 Loss: 0.08580334484577179\n",
            "Epoch: 3491 Loss: 0.07582389563322067\n",
            "Epoch: 3501 Loss: 0.14285296201705933\n",
            "Epoch: 3511 Loss: 0.0642724335193634\n",
            "Epoch: 3521 Loss: 0.0618126317858696\n",
            "Epoch: 3531 Loss: 0.09157172590494156\n",
            "Epoch: 3541 Loss: 0.1543799787759781\n",
            "Epoch: 3551 Loss: 0.3436828851699829\n",
            "Epoch: 3561 Loss: 0.12350020557641983\n",
            "Epoch: 3571 Loss: 0.08552758395671844\n",
            "Epoch: 3581 Loss: 0.0757097452878952\n",
            "Epoch: 3591 Loss: 0.04626263678073883\n",
            "Epoch: 3601 Loss: 0.08203672617673874\n",
            "Epoch: 3611 Loss: 0.12512223422527313\n",
            "Epoch: 3621 Loss: 0.09093357622623444\n",
            "Epoch: 3631 Loss: 0.11373064666986465\n",
            "Epoch: 3641 Loss: 0.09288768470287323\n",
            "Epoch: 3651 Loss: 0.037439703941345215\n",
            "Epoch: 3661 Loss: 0.05329190567135811\n",
            "Epoch: 3671 Loss: 0.10594239085912704\n",
            "Epoch: 3681 Loss: 0.17386020720005035\n",
            "Epoch: 3691 Loss: 0.017938964068889618\n",
            "Epoch: 3701 Loss: 0.1340731829404831\n",
            "Epoch: 3711 Loss: 0.07780696451663971\n",
            "Epoch: 3721 Loss: 0.10930856317281723\n",
            "Epoch: 3731 Loss: 0.2580057978630066\n",
            "Epoch: 3741 Loss: 0.03583388775587082\n",
            "Epoch: 3751 Loss: 0.1163153201341629\n",
            "Epoch: 3761 Loss: 0.08964981138706207\n",
            "Epoch: 3771 Loss: 0.19353684782981873\n",
            "Epoch: 3781 Loss: 0.14694955945014954\n",
            "Epoch: 3791 Loss: 0.04627666622400284\n",
            "Epoch: 3801 Loss: 0.15294162929058075\n",
            "Epoch: 3811 Loss: 0.07105546444654465\n",
            "Epoch: 3821 Loss: 0.11764176189899445\n",
            "Epoch: 3831 Loss: 0.11643590033054352\n",
            "Epoch: 3841 Loss: 0.1630500853061676\n",
            "Epoch: 3851 Loss: 0.060187049210071564\n",
            "Epoch: 3861 Loss: 0.09637677669525146\n",
            "Epoch: 3871 Loss: 0.08677615970373154\n",
            "Epoch: 3881 Loss: 0.05984734371304512\n",
            "Epoch: 3891 Loss: 0.13544151186943054\n",
            "Epoch: 3901 Loss: 0.07486617565155029\n",
            "Epoch: 3911 Loss: 0.042017240077257156\n",
            "Epoch: 3921 Loss: 0.08659548312425613\n",
            "Epoch: 3931 Loss: 0.08078906685113907\n",
            "Epoch: 3941 Loss: 0.14502686262130737\n",
            "Epoch: 3951 Loss: 0.13785889744758606\n",
            "Epoch: 3961 Loss: 0.1631777435541153\n",
            "Epoch: 3971 Loss: 0.22217991948127747\n",
            "Epoch: 3981 Loss: 0.10484234243631363\n",
            "Epoch: 3991 Loss: 0.10107390582561493\n",
            "Epoch: 4001 Loss: 0.029810762032866478\n",
            "Epoch: 4011 Loss: 0.09126275032758713\n",
            "Epoch: 4021 Loss: 0.06652047485113144\n",
            "Epoch: 4031 Loss: 0.023079369217157364\n",
            "Epoch: 4041 Loss: 0.11950190365314484\n",
            "Epoch: 4051 Loss: 0.14646252989768982\n",
            "Epoch: 4061 Loss: 0.4385469853878021\n",
            "Epoch: 4071 Loss: 0.033494047820568085\n",
            "Epoch: 4081 Loss: 0.04716286435723305\n",
            "Epoch: 4091 Loss: 0.09855493903160095\n",
            "Epoch: 4101 Loss: 0.11406335979700089\n",
            "Epoch: 4111 Loss: 0.09656869620084763\n",
            "Epoch: 4121 Loss: 0.020281367003917694\n",
            "Epoch: 4131 Loss: 0.09301134198904037\n",
            "Epoch: 4141 Loss: 0.10275699943304062\n",
            "Epoch: 4151 Loss: 0.07692502439022064\n",
            "Epoch: 4161 Loss: 0.11669036000967026\n",
            "Epoch: 4171 Loss: 0.08558301627635956\n",
            "Epoch: 4181 Loss: 0.06957371532917023\n",
            "Epoch: 4191 Loss: 0.17646799981594086\n",
            "Epoch: 4201 Loss: 0.03414011746644974\n",
            "Epoch: 4211 Loss: 0.20265324413776398\n",
            "Epoch: 4221 Loss: 0.10738060623407364\n",
            "Epoch: 4231 Loss: 0.06016581133008003\n",
            "Epoch: 4241 Loss: 0.12289869785308838\n",
            "Epoch: 4251 Loss: 0.05325143784284592\n",
            "Epoch: 4261 Loss: 0.0563502237200737\n",
            "Epoch: 4271 Loss: 0.08229396492242813\n",
            "Epoch: 4281 Loss: 0.34389445185661316\n",
            "Epoch: 4291 Loss: 0.07793077081441879\n",
            "Epoch: 4301 Loss: 0.15762503445148468\n",
            "Epoch: 4311 Loss: 0.1244693398475647\n",
            "Epoch: 4321 Loss: 0.06255221366882324\n",
            "Epoch: 4331 Loss: 0.07148095965385437\n",
            "Epoch: 4341 Loss: 0.05445099622011185\n",
            "Epoch: 4351 Loss: 0.047093458473682404\n",
            "Epoch: 4361 Loss: 0.06631050258874893\n",
            "Epoch: 4371 Loss: 0.057716261595487595\n",
            "Epoch: 4381 Loss: 0.1068718284368515\n",
            "Epoch: 4391 Loss: 0.047283194959163666\n",
            "Epoch: 4401 Loss: 0.05344840884208679\n",
            "Epoch: 4411 Loss: 0.37670761346817017\n",
            "Epoch: 4421 Loss: 0.21630409359931946\n",
            "Epoch: 4431 Loss: 0.051835738122463226\n",
            "Epoch: 4441 Loss: 0.06238935515284538\n",
            "Epoch: 4451 Loss: 0.07412154972553253\n",
            "Epoch: 4461 Loss: 0.03188059851527214\n",
            "Epoch: 4471 Loss: 0.10942260175943375\n",
            "Epoch: 4481 Loss: 0.05264804884791374\n",
            "Epoch: 4491 Loss: 0.08554674685001373\n",
            "Epoch: 4501 Loss: 0.06736216694116592\n",
            "Epoch: 4511 Loss: 0.11863914877176285\n",
            "Epoch: 4521 Loss: 0.06041157245635986\n",
            "Epoch: 4531 Loss: 0.07602640241384506\n",
            "Epoch: 4541 Loss: 0.08453985303640366\n",
            "Epoch: 4551 Loss: 0.06057234853506088\n",
            "Epoch: 4561 Loss: 0.05525487661361694\n",
            "Epoch: 4571 Loss: 0.03881338983774185\n",
            "Epoch: 4581 Loss: 0.021520491689443588\n",
            "Epoch: 4591 Loss: 0.0811769962310791\n",
            "Epoch: 4601 Loss: 0.0932222455739975\n",
            "Epoch: 4611 Loss: 0.1127605065703392\n",
            "Epoch: 4621 Loss: 0.04777756705880165\n",
            "Epoch: 4631 Loss: 0.11493480205535889\n",
            "Epoch: 4641 Loss: 0.11104588210582733\n",
            "Epoch: 4651 Loss: 0.12433274835348129\n",
            "Epoch: 4661 Loss: 0.12237711995840073\n",
            "Epoch: 4671 Loss: 0.0971401110291481\n",
            "Epoch: 4681 Loss: 0.0742383524775505\n",
            "Epoch: 4691 Loss: 0.14436864852905273\n",
            "Epoch: 4701 Loss: 0.03566765412688255\n",
            "Epoch: 4711 Loss: 0.11740126460790634\n",
            "Epoch: 4721 Loss: 0.09251381456851959\n",
            "Epoch: 4731 Loss: 0.3075248599052429\n",
            "Epoch: 4741 Loss: 0.15249204635620117\n",
            "Epoch: 4751 Loss: 0.06900415569543839\n",
            "Epoch: 4761 Loss: 0.0457506999373436\n",
            "Epoch: 4771 Loss: 0.0983673706650734\n",
            "Epoch: 4781 Loss: 0.13729195296764374\n",
            "Epoch: 4791 Loss: 0.12310860306024551\n",
            "Epoch: 4801 Loss: 0.04815015196800232\n",
            "Epoch: 4811 Loss: 0.060073621571063995\n",
            "Epoch: 4821 Loss: 0.20803424715995789\n",
            "Epoch: 4831 Loss: 0.14032891392707825\n",
            "Epoch: 4841 Loss: 0.20787420868873596\n",
            "Epoch: 4851 Loss: 0.05235256627202034\n",
            "Epoch: 4861 Loss: 0.08359378576278687\n",
            "Epoch: 4871 Loss: 0.036090023815631866\n",
            "Epoch: 4881 Loss: 0.04255444556474686\n",
            "Epoch: 4891 Loss: 0.11161712557077408\n",
            "Epoch: 4901 Loss: 0.050124138593673706\n",
            "Epoch: 4911 Loss: 0.023824896663427353\n",
            "Epoch: 4921 Loss: 0.15408681333065033\n",
            "Epoch: 4931 Loss: 0.04341980814933777\n",
            "Epoch: 4941 Loss: 0.20793218910694122\n",
            "Epoch: 4951 Loss: 0.024008071050047874\n",
            "Epoch: 4961 Loss: 0.05513644963502884\n",
            "Epoch: 4971 Loss: 0.0925394743680954\n",
            "Epoch: 4981 Loss: 0.3606853485107422\n",
            "Epoch: 4991 Loss: 0.06113965064287186\n",
            "Final loss: 0.021671012043952942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy = {}\n",
        "for mover_position in range(1, 50):\n",
        "  for mover_action in range(4):\n",
        "    state = (mover_position, mover_action)\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "    q_values = model(state_tensor)\n",
        "    best_action = torch.argmax(q_values).item()\n",
        "    policy[state] = best_action\n",
        "print(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcjya0YcQnEN",
        "outputId": "e875bece-68e3-44af-cb60-0f611e5bd226"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(1, 0): 7, (1, 1): 7, (1, 2): 7, (1, 3): 7, (2, 0): 7, (2, 1): 7, (2, 2): 7, (2, 3): 7, (3, 0): 7, (3, 1): 7, (3, 2): 7, (3, 3): 7, (4, 0): 7, (4, 1): 7, (4, 2): 7, (4, 3): 7, (5, 0): 7, (5, 1): 7, (5, 2): 7, (5, 3): 7, (6, 0): 7, (6, 1): 7, (6, 2): 7, (6, 3): 7, (7, 0): 7, (7, 1): 7, (7, 2): 7, (7, 3): 7, (8, 0): 7, (8, 1): 7, (8, 2): 7, (8, 3): 7, (9, 0): 7, (9, 1): 7, (9, 2): 7, (9, 3): 7, (10, 0): 7, (10, 1): 7, (10, 2): 7, (10, 3): 7, (11, 0): 7, (11, 1): 7, (11, 2): 7, (11, 3): 7, (12, 0): 5, (12, 1): 5, (12, 2): 5, (12, 3): 5, (13, 0): 5, (13, 1): 5, (13, 2): 5, (13, 3): 5, (14, 0): 5, (14, 1): 5, (14, 2): 5, (14, 3): 5, (15, 0): 5, (15, 1): 5, (15, 2): 5, (15, 3): 5, (16, 0): 5, (16, 1): 5, (16, 2): 5, (16, 3): 5, (17, 0): 5, (17, 1): 5, (17, 2): 5, (17, 3): 5, (18, 0): 5, (18, 1): 5, (18, 2): 5, (18, 3): 5, (19, 0): 5, (19, 1): 5, (19, 2): 5, (19, 3): 5, (20, 0): 5, (20, 1): 5, (20, 2): 5, (20, 3): 5, (21, 0): 5, (21, 1): 5, (21, 2): 5, (21, 3): 5, (22, 0): 5, (22, 1): 5, (22, 2): 5, (22, 3): 5, (23, 0): 5, (23, 1): 5, (23, 2): 5, (23, 3): 5, (24, 0): 5, (24, 1): 5, (24, 2): 5, (24, 3): 5, (25, 0): 5, (25, 1): 5, (25, 2): 5, (25, 3): 5, (26, 0): 5, (26, 1): 5, (26, 2): 5, (26, 3): 5, (27, 0): 5, (27, 1): 5, (27, 2): 5, (27, 3): 5, (28, 0): 5, (28, 1): 5, (28, 2): 5, (28, 3): 5, (29, 0): 5, (29, 1): 5, (29, 2): 5, (29, 3): 5, (30, 0): 5, (30, 1): 5, (30, 2): 5, (30, 3): 5, (31, 0): 5, (31, 1): 5, (31, 2): 5, (31, 3): 5, (32, 0): 0, (32, 1): 5, (32, 2): 5, (32, 3): 5, (33, 0): 0, (33, 1): 5, (33, 2): 5, (33, 3): 5, (34, 0): 0, (34, 1): 5, (34, 2): 5, (34, 3): 5, (35, 0): 0, (35, 1): 5, (35, 2): 5, (35, 3): 5, (36, 0): 0, (36, 1): 5, (36, 2): 5, (36, 3): 5, (37, 0): 0, (37, 1): 5, (37, 2): 5, (37, 3): 5, (38, 0): 0, (38, 1): 5, (38, 2): 5, (38, 3): 5, (39, 0): 0, (39, 1): 5, (39, 2): 5, (39, 3): 8, (40, 0): 5, (40, 1): 5, (40, 2): 8, (40, 3): 8, (41, 0): 5, (41, 1): 4, (41, 2): 8, (41, 3): 8, (42, 0): 2, (42, 1): 8, (42, 2): 8, (42, 3): 8, (43, 0): 8, (43, 1): 8, (43, 2): 8, (43, 3): 8, (44, 0): 8, (44, 1): 8, (44, 2): 8, (44, 3): 8, (45, 0): 8, (45, 1): 8, (45, 2): 8, (45, 3): 8, (46, 0): 8, (46, 1): 8, (46, 2): 8, (46, 3): 8, (47, 0): 8, (47, 1): 8, (47, 2): 8, (47, 3): 8, (48, 0): 8, (48, 1): 8, (48, 2): 8, (48, 3): 3, (49, 0): 8, (49, 1): 8, (49, 2): 3, (49, 3): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = EaterEnvironment(mover_policy)\n",
        "state = env.reset()\n",
        "\n",
        "for i in range(19):\n",
        "    if state[1] == 4:\n",
        "      print(f\" Final goal : {state[0]} Bananas at g1: {env.bananas_g1}, Bananas at g2: {env.bananas_g2}\")\n",
        "      break\n",
        "    action_index = policy[state] # Cycle through actions\n",
        "    next_state, reward, done = env.step(state, action_index)\n",
        "    if done != True:\n",
        "      print(f\"Step {i+1}: Next State: {next_state}, Reward: {reward}, action: {env.actions[action_index]} Bananas at g1: {env.bananas_g1}, Bananas at g2: {env.bananas_g2}\")\n",
        "    state = next_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUjf4a9xcXfk",
        "outputId": "df5e4435-318f-4e85-8e5e-16efc9a96f6b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Next State: (8, 0), Reward: 0.9750000000000001, action: (0.7, 0.3) Bananas at g1: 99.3, Bananas at g2: 99.7\n",
            "Step 2: Next State: (15, 0), Reward: 0.9714285714285713, action: (0.7, 0.3) Bananas at g1: 98.6, Bananas at g2: 99.4\n",
            "Step 3: Next State: (22, 0), Reward: 0.8999999999999999, action: (0.8, 0.2) Bananas at g1: 97.8, Bananas at g2: 99.2\n",
            "Step 4: Next State: (29, 0), Reward: 1.0, action: (0.8, 0.2) Bananas at g1: 97.0, Bananas at g2: 99.0\n",
            "Step 5: Next State: (36, 0), Reward: 0.8500000000000001, action: (0.8, 0.2) Bananas at g1: 96.2, Bananas at g2: 98.8\n",
            "Step 6: Next State: (37, 3), Reward: 1.0, action: (1, 0) Bananas at g1: 95.2, Bananas at g2: 98.8\n",
            "Step 7: Next State: (38, 3), Reward: 0.9333333333333333, action: (0.8, 0.2) Bananas at g1: 94.4, Bananas at g2: 98.6\n",
            "Step 8: Next State: (39, 3), Reward: 0.7333333333333332, action: (0.8, 0.2) Bananas at g1: 94.4, Bananas at g2: 97.6\n",
            "Step 9: Next State: (46, 0), Reward: 0.6000000000000001, action: (0.3, 0.7) Bananas at g1: 94.4, Bananas at g2: 96.6\n",
            "Step 10: Next State: (47, 3), Reward: 0.9333333333333335, action: (0.3, 0.7) Bananas at g1: 94.4, Bananas at g2: 95.6\n",
            "Step 11: Next State: (48, 3), Reward: 0.7333333333333333, action: (0.3, 0.7) Bananas at g1: 94.4, Bananas at g2: 94.6\n",
            "Step 12: Next State: (49, 3), Reward: 0.6000000000000001, action: (0.2, 0.8) Bananas at g1: 94.4, Bananas at g2: 93.6\n",
            " Final goal : 49 Bananas at g1: 94.4, Bananas at g2: 92.6\n"
          ]
        }
      ]
    }
  ]
}